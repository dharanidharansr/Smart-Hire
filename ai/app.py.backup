import re
import uuid
import io
import json
import traceback
import PyPDF2
import spacy
from spacy.matcher import Matcher, PhraseMatcher
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks, Request, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, validator 
from typing import List, Optional, Dict, Any
import os
from dotenv import load_dotenv
from datetime import datetime, timedelta
from groq import Groq
from functools import lru_cache
from pathlib import Path

from typing import Dict, Any

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from pydantic import BaseModel

# Import MongoDB client
from mongodb_client import mongodb, init_database, close_database

# Global variable to track database connection status
database_connected = False


# Load environment variables
load_dotenv()

# Initialize Groq
# Available current models (as of 2025):
# - llama-3.1-8b-instant (fast, good quality)
# - llama-3.1-70b-versatile (decommissioned - do not use)
# - mixtral-8x7b-32768 (good for longer contexts)
# - gemma-7b-it (Google's Gemma model)
# - llama3-groq-8b-8192-tool-use-preview (for tool use)
# Check https://console.groq.com/docs/models for latest available models

client = None
try:
    groq_api_key = os.getenv("GROQ_API_KEY")
    if groq_api_key and groq_api_key != "your_groq_api_key_here":
        client = Groq(api_key=groq_api_key)
        print("âœ… Groq configured successfully")
    else:
        print("âš ï¸ Groq API key not configured - AI features will be limited")
except Exception as e:
    print(f"âŒ Groq configuration failed: {e}")
    print("âš ï¸ Continuing without Groq - AI features will be limited")

# Initialize FastAPI
app = FastAPI(
    title="AI Resume Parser with MongoDB Backend",
    version="1.0",
    debug=True
)

# Add startup and shutdown events
@app.on_event("startup")
async def on_startup():
    await startup_event()

@app.on_event("shutdown")
async def on_shutdown():
    await shutdown_event()

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class RawResumeData(BaseModel):
    raw_text: str
    processed_data: Dict[str, Any]

async def extract_raw_text_from_pdf(file: UploadFile) -> str:
    """Extract raw text from PDF without any processing"""
    try:
        if file.content_type != "application/pdf":
            raise ValueError("Only PDF files are accepted")
            
        contents = await file.read()
        if not contents:
            raise ValueError("Empty file uploaded")
            
        if len(contents) > 5 * 1024 * 1024:
            raise ValueError("File too large (max 5MB)")
            
        pdf_file = io.BytesIO(contents)
        reader = PyPDF2.PdfReader(pdf_file)
        if len(reader.pages) == 0:
            raise ValueError("PDF contains no pages")
            
        text = "\n".join([page.extract_text() or "" for page in reader.pages])
        if not text.strip():
            raise ValueError("No text could be extracted from PDF")
            
        file.file.seek(0)
        return text
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"PDF processing failed: {str(e)}")

async def process_with_groq(raw_text: str) -> Dict[str, Any]:
    """Send raw text to Gemini for complete processing"""
    prompt = f"""
    Analyze this raw resume text and extract all relevant information.
    Return a comprehensive JSON structure containing:
    - Personal information (name, email, phone)
    - Education history (degrees, institutions, years)
    - Work experience (companies, positions, durations)
    - Skills (technical and soft skills)
    - Certifications
    - Any other relevant information
    
    Also calculate an ATS (Applicant Tracking System) score from 0-100 based on:
    - Resume formatting and structure
    - Keyword relevance and density
    - Completeness of information
    - Professional language use
    - Overall quality and readability
    
    Structure the output in a professional, standardized format.
    Correct any typos or inconsistencies you find.
    
    RAW RESUME TEXT:
    {raw_text[:10000]}  # First 10,000 characters to avoid token limits
    
    Return ONLY the JSON output. Do not include any additional text or explanations.
    The JSON should be properly formatted with all fields correctly named.
    Include the ATS score in a field called "ats_score".
    """
    
    try:
        if not client:
            raise HTTPException(
                status_code=503,
                detail="Groq AI service not available. Please try basic parsing instead."
            )
        
        response = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "You are an expert resume parser. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=4000
        )
        
        json_str = response.choices[0].message.content
        # Clean the response if it contains markdown code blocks
        if "```json" in json_str:
            json_str = json_str.split("```json")[1].split("```")[0].strip()
        elif "```" in json_str:
            json_str = json_str.split("```")[1].split("```")[0].strip()
        
        # Find JSON object
        start_idx = json_str.find('{')
        end_idx = json_str.rfind('}') + 1
        if start_idx != -1 and end_idx != 0:
            json_str = json_str[start_idx:end_idx]
        
        return json.loads(json_str)
    except Exception as e:
        error_msg = str(e)
        print(f"Groq processing error: {error_msg}")
        print(f"Raw response: {json_str if 'json_str' in locals() else 'No response'}")
        if "quota" in error_msg.lower() or "429" in error_msg or "rate limit" in error_msg.lower():
            raise HTTPException(
                status_code=429,
                detail="AI service quota exceeded. Please try again later or use basic parsing features."
            )
        raise HTTPException(
            status_code=500,
            detail=f"Gemini processing failed: {error_msg}"
        )
    
RESUMES_JSON_FILE = "resumes_data.json"

def initialize_resumes_file():
    """Create the JSON file if it doesn't exist with an empty list"""
    if not Path(RESUMES_JSON_FILE).exists():
        with open(RESUMES_JSON_FILE, 'w') as f:
            json.dump([], f)

def append_resume_to_file(new_resume: Dict[str, Any]) -> bool:
    """
    Append a new resume to the JSON file if it doesn't already exist.
    Returns True if resume was added, False if it already existed.
    """
    try:
        # Read existing data
        with open(RESUMES_JSON_FILE, 'r') as f:
            existing_data: List[Dict[str, Any]] = json.load(f)
        
        if "email" in new_resume and new_resume["email"]:
            for existing_resume in existing_data:
                if "email" in existing_resume and existing_resume["email"] == new_resume["email"]:
                    print(f"Resume with email {new_resume['email']} already exists. Skipping.")
                    return False
        
        # As a fallback, also check by name if email is missing
        elif "name" in new_resume and new_resume["name"]:
            for existing_resume in existing_data:
                if "name" in existing_resume and existing_resume["name"] == new_resume["name"]:
                    print(f"Resume with name {new_resume['name']} already exists. Skipping.")
                    return False
        
        # Append new resume only if it doesn't exist
        existing_data.append(new_resume)
        
        # Write back to file
        with open(RESUMES_JSON_FILE, 'w') as f:
            json.dump(existing_data, f, indent=2)
        
        print(f"Added new resume to file: {new_resume.get('email', new_resume.get('name', 'Unknown'))}")
        return True
            
    except Exception as e:
        print(f"Failed to update resumes file: {str(e)}")
        raise Exception(f"Failed to update resumes file: {str(e)}")

# Initialize the file when module loads
initialize_resumes_file()

# Authentication Endpoints
@app.post("/auth/signup", response_model=UserResponse)
async def sign_up(user_data: UserSignUp):
    """User registration endpoint"""
    try:
        if not database_connected:
            raise HTTPException(status_code=500, detail="Database not connected")
        
        # Check if user already exists
        existing_user = await mongodb.find_one("users", {"email": user_data.email})
        if existing_user:
            raise HTTPException(status_code=400, detail="User with this email already exists")
        
        # Create new user
        user_id = str(uuid.uuid4())
        hashed_password = hash_password(user_data.password)
        
        new_user = {
            "id": user_id,
            "email": user_data.email,
            "password": hashed_password,
            "first_name": user_data.first_name,
            "last_name": user_data.last_name,
            "role": user_data.role,
            "company_id": user_data.company_id,
            "company_name": user_data.company_name,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        # Save to MongoDB
        await mongodb.insert_one("users", new_user)
        print(f"âœ… User created: {user_data.email} with role {user_data.role}")
        
        # Return user data (without password)
        return UserResponse(
            id=user_id,
            email=user_data.email,
            first_name=user_data.first_name,
            last_name=user_data.last_name,
            role=user_data.role,
            created_at=new_user["created_at"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Sign up error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Registration failed: {str(e)}")

@app.post("/auth/signin")
async def sign_in(credentials: UserSignIn):
    """User authentication endpoint"""
    try:
        if not database_connected:
            raise HTTPException(status_code=500, detail="Database not connected")
        
        # Find user by email
        user = await mongodb.find_one("users", {"email": credentials.email})
        if not user:
            raise HTTPException(status_code=401, detail="Invalid email or password")
        
        # Check password
        hashed_password = hash_password(credentials.password)
        if user["password"] != hashed_password:
            raise HTTPException(status_code=401, detail="Invalid email or password")
        
        # Generate session token
        token = generate_token()
        
        # Update user with last login
        await mongodb.update_one("users", 
                                {"id": user["id"]}, 
                                {"last_login": datetime.now().isoformat()})
        
        print(f"âœ… User signed in: {credentials.email}")
        
        # Return user data and token
        return {
            "user": {
                "id": user["id"],
                "email": user["email"],
                "first_name": user.get("first_name"),
                "last_name": user.get("last_name"),
                "role": user["role"],
                "company_id": user.get("company_id"),
                "company_name": user.get("company_name"),
                "created_at": user["created_at"]
            },
            "token": token
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Sign in error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Authentication failed: {str(e)}")

# Resume Processing Endpoints
@app.post("/send-data", response_model=RawResumeData)
async def send_data(file: UploadFile = File(...)):
    """Endpoint that sends raw resume data to Groq for processing"""
    try:
        # 1. Extract raw text from PDF
        raw_text = await extract_raw_text_from_pdf(file)
        
        # 2. Send directly to Groq for processing
        raw_processed_data = await process_with_groq(raw_text)
        
        # 3. Normalize the data to consistent format
        processed_data = normalize_resume_data(raw_processed_data)
        
        # 4. Append to JSON file if not already exists (legacy support)
        was_added_to_file = append_resume_to_file(processed_data)
        
        # 5. Also save to MongoDB if connected
        was_added_to_db = False
        if database_connected:
            try:
                # Check if resume already exists in MongoDB
                existing_resume = None
                email = processed_data.get("personal_info", {}).get("email")
                name = processed_data.get("personal_info", {}).get("name")
                
                if email:
                    existing_resume = await mongodb.find_one("extracted_resume_data", {"personal_info.email": email})
                elif name:
                    existing_resume = await mongodb.find_one("extracted_resume_data", {"personal_info.name": name})
                
                if not existing_resume:
                    # Add metadata
                    resume_doc = {
                        **processed_data,
                        "raw_text": raw_text,
                        "uploaded_at": datetime.now().isoformat(),
                        "filename": file.filename,
                        "id": str(uuid.uuid4()),
                        "created_at": datetime.now().isoformat()
                    }
                    
                    # Save to MongoDB
                    await mongodb.insert_one("extracted_resume_data", resume_doc)
                    print(f"âœ… Resume saved to MongoDB: {email or name or 'Unknown'}")
                    was_added_to_db = True
                else:
                    print(f"Resume already exists in MongoDB: {email or name or 'Unknown'}")
                    
            except Exception as e:
                print(f"Failed to save resume to MongoDB: {str(e)}")
        
        response_data = RawResumeData(
            raw_text=raw_text[:1000] + "... [truncated]",
            processed_data=processed_data
        )
        
        # Add a note about whether the resume was added or already existed
        if not was_added_to_file and not was_added_to_db:
            response_data.processed_data["note"] = "Resume already exists in database"
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Resume processing failed: {str(e)}"
        )
# Load spaCy model
try:
    nlp = spacy.load("en_core_web_sm")
    print("âœ… spaCy model loaded successfully")
except Exception as e:
    print(f"âŒ spaCy model loading failed: {e}")
    raise

# Initialize MongoDB
database_connected = False
async def startup_event():
    """Initialize database connection on startup"""
    global database_connected
    database_connected = await init_database()
    if database_connected:
        print("âœ… MongoDB connection successful")
    else:
        print("âš ï¸ MongoDB not configured - running without database")

async def shutdown_event():
    """Close database connection on shutdown"""
    await close_database()

# Constants
SKILL_BLACKLIST = {
    "and", "the", "with", "using", "via", "from", "to", 
    "in", "on", "at", "for", "my", "our", "we", "i",
    "|", "", " ", "  ", "-", "â€¢", ":", ";", ",", "."
}

TECHNICAL_SKILLS = [
    "Python", "Java", "JavaScript", "C++", "SQL", "NoSQL", 
    "HTML", "CSS", "React", "Angular", "Node.js", "Django",
    "Flask", "TensorFlow", "PyTorch", "Machine Learning",
    "Data Science", "Data Analysis", "Android Development",
    "IoT", "Cloud Computing", "AWS", "Azure", "Git",
    "REST API", "GraphQL", "Docker", "Kubernetes",
    "Computer Engineering", "Research", "Web Development"
]

# Models
class EnhancedResumeData(BaseModel):
    resume_text: str
    job_description: str
    education: str
    industry: str
    applied_job_title: str
    experience_years: float
    salary_expectation: float
    skills: List[str]
    corrections_made: List[str] = Field(default_factory=list)

class ResumeData(BaseModel):
    name: str
    email: Optional[str] = None
    phone: Optional[str] = None
    resume_text: str
    extracted_skills: List[str]
    work_experience: List[Dict[str, Any]]
    education: List[Dict[str, Any]]
    certifications: Optional[List[str]] = None
    enhanced_data: Optional[EnhancedResumeData] = None

class JobApplication(BaseModel):
    job_id: str
    candidate_id: str

class JobWithCandidates(BaseModel):
    job: Dict[str, Any]
    candidates: List[ResumeData]

# Authentication models
class UserSignUp(BaseModel):
    email: str  # Changed from EmailStr to str to avoid validation issues
    password: str
    first_name: Optional[str] = None
    last_name: Optional[str] = None
    role: str = "candidate"  # candidate or hr_user
    company_id: Optional[str] = None
    company_name: Optional[str] = None

class UserSignIn(BaseModel):
    email: str  # Changed from EmailStr to str to avoid validation issues
    password: str

class UserResponse(BaseModel):
    id: str
    email: str
    first_name: Optional[str] = None
    last_name: Optional[str] = None
    role: str
    created_at: str

# Simple JWT-like token (in production, use proper JWT)
import hashlib
import secrets

def hash_password(password: str) -> str:
    """Simple password hashing"""
    return hashlib.sha256(password.encode()).hexdigest()

def generate_token() -> str:
    """Generate a simple session token"""
    return secrets.token_urlsafe(32)

# Helper Functions
def initialize_nlp_components():
    skill_patterns = list(nlp.pipe([
        skill for skill in TECHNICAL_SKILLS 
        if len(skill.split()) < 3
    ]))
    skill_matcher = PhraseMatcher(nlp.vocab)
    skill_matcher.add("SKILL", skill_patterns)

    experience_matcher = Matcher(nlp.vocab)
    experience_patterns = [
        [{"POS": "PROPN", "OP": "+"}, {"LOWER": "at"}, {"POS": "PROPN", "OP": "+"}],
        [{"POS": "PROPN", "OP": "+"}, {"LOWER": ","}, {"LOWER": "inc"}],
        [{"POS": "PROPN", "OP": "+"}, {"LOWER": "company"}],
    ]
    experience_matcher.add("EXPERIENCE", experience_patterns)

    return skill_matcher, experience_matcher

skill_matcher, experience_matcher = initialize_nlp_components()

def is_valid_skill(text):
    return (
        len(text) > 2 and
        not any(char.isdigit() for char in text) and
        text.lower() not in SKILL_BLACKLIST and
        not text.isspace() and
        not text.endswith((".", ",", ";", ":"))
    )

def extract_skills(doc) -> List[str]:
    skills = set()
    matches = skill_matcher(doc)
    for match_id, start, end in matches:
        skill = doc[start:end].text
        if is_valid_skill(skill):
            skills.add(skill)
    
    for sent in doc.sents:
        if any(kw in sent.text.lower() for kw in ["skill", "experience", "proficient"]):
            for token in sent:
                if (token.pos_ in ["NOUN", "PROPN"] and is_valid_skill(token.text)):
                    skills.add(token.text)
    
    return sorted({re.sub(r'[^\w\s]', '', s).strip() for s in skills if is_valid_skill(s)})

def extract_from_section(doc, section_title):
    section_content = []
    found_section = False
    for sent in doc.sents:
        if section_title.lower() in sent.text.lower():
            found_section = True
            continue
        if found_section and any(kw in sent.text.lower() for kw in ["experience", "education"]):
            break
        if found_section:
            section_content.append(sent.text)
    return " ".join(section_content)

async def extract_text_from_pdf(file: UploadFile) -> str:
    try:
        if file.content_type != "application/pdf":
            raise ValueError("Only PDF files are accepted")
            
        contents = await file.read()
        if not contents:
            raise ValueError("Empty file uploaded")
            
        if len(contents) > 5 * 1024 * 1024:
            raise ValueError("File too large (max 5MB)")
            
        pdf_file = io.BytesIO(contents)
        reader = PyPDF2.PdfReader(pdf_file)
        if len(reader.pages) == 0:
            raise ValueError("PDF contains no pages")
            
        text = "\n".join([page.extract_text() or "" for page in reader.pages])
        if not text.strip():
            raise ValueError("No text could be extracted from PDF")
            
        file.file.seek(0)
        return text
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"PDF processing failed: {str(e)}")

async def upload_resume_to_storage(file: UploadFile) -> str:
    """
    Mock file upload - returns a placeholder URL
    In production, you would upload to a cloud storage service like AWS S3, Google Cloud Storage, etc.
    """
    try:
        file_extension = os.path.splitext(file.filename)[1]
        unique_filename = f"{uuid.uuid4()}{file_extension}"
        
        # For now, return a placeholder URL
        # In production, implement actual file upload to your preferred storage service
        placeholder_url = f"https://storage.example.com/resumes/{unique_filename}"
        
        return placeholder_url
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to upload resume: {str(e)}")

def extract_contact_info(doc) -> Dict[str, str]:
    contact_info = {"name": "", "email": "", "phone": ""}
    for ent in doc.ents:
        if ent.label_ == "PERSON":
            contact_info["name"] = ent.text
            break
    
    emails = re.findall(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", doc.text)
    if emails:
        contact_info["email"] = emails[0]
    
    phones = re.findall(r"(\+?\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}", doc.text)
    if phones:
        contact_info["phone"] = phones[0]
    
    return contact_info

def extract_experience(doc) -> List[Dict[str, Any]]:
    experiences = []
    current_position = None
    current_dates = None
    
    for sent in doc.sents:
        if (" at " in sent.text or " for " in sent.text or 
            " intern " in sent.text.lower() or "internship" in sent.text.lower()):
            position = sent.text.split(" at ")[0] if " at " in sent.text else sent.text
            current_position = position.split(" for ")[0] if " for " in position else position
        
        date_matches = re.findall(
            r"((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]\s\d{4}|\d{4}).?((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s\d{4}|\d{4}|Present|Current)", 
            sent.text, 
            re.IGNORECASE
        )
        if date_matches:
            current_dates = f"{date_matches[0][0]} - {date_matches[0][2]}"
        
        if current_position and current_dates:
            experiences.append({
                "position": current_position.strip(),
                "duration": current_dates,
                "description": sent.text,
                "type": "internship" if "intern" in current_position.lower() else "work"
            })
            current_position = current_dates = None
    
    return experiences

def extract_education(doc) -> List[Dict[str, Any]]:
    education = []
    current_degree = None
    current_school = None
    
    for sent in doc.sents:
        if any(kw in sent.text for kw in ["Bachelor", "Master", "PhD", "Doctorate"]):
            current_degree = sent.text.split(" in ")[0] if " in " in sent.text else sent.text
        
        if any(kw in sent.text for kw in ["University", "College", "Institute"]):
            current_school = sent.text
        
        if current_degree and current_school:
            education.append({
                "degree": current_degree,
                "institution": current_school
            })
            current_degree = current_school = None
    
    return education

async def enhance_resume_with_groq(raw_text: str, extracted_data: dict) -> EnhancedResumeData:
    print(f"ðŸ” Enhancing resume with Groq...")
    print(f"ðŸ“ Raw text length: {len(raw_text)}")
    print(f"ðŸ“Š Extracted data keys: {list(extracted_data.keys())}")
    
    # Safely convert extracted_data to JSON
    try:
        json_str = json.dumps(extracted_data, indent=2, default=str)
        print(f"âœ… JSON conversion successful")
    except Exception as json_error:
        print(f"âŒ JSON conversion failed: {json_error}")
        json_str = str(extracted_data)
    
    prompt = f"""
    Analyze this resume and enhance the extracted data:
    
    RAW TEXT EXCERPT:
    {raw_text[:3000]}
    
    CURRENT EXTRACTION:
    {json_str}
    
    Please return enhanced data in this exact format:
    {{
        "resume_text": "Enhanced professional summary",
        "job_description": "Matched job description",
        "education": "Standardized education",
        "industry": "Detected industry",
        "applied_job_title": "Suggested job title",
        "experience_years": X.X,
        "salary_expectation": XXXXX.X,
        "skills": ["Standardized", "Skills"],
        "corrections_made": ["List of corrections"]
    }}
    """
    
    try:
        if not client:
            print("Groq not available, using fallback data")
            return EnhancedResumeData(
                resume_text=extracted_data.get("resume_text", ""),
                job_description="",
                education=", ".join([edu.get("degree", "") for edu in extracted_data.get("education", [])]),
                industry="Technology", # Default industry
                applied_job_title="Software Developer", # Default title
                experience_years=len(extracted_data.get("work_experience", [])),
                salary_expectation=0,
                skills=extracted_data.get("extracted_skills", []),
                corrections_made=["Using basic parsing - Groq not available"]
            )
        
        response = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "You are an expert resume analyzer. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=2000
        )
        
        json_str = response.choices[0].message.content
        # Clean the response if it contains markdown code blocks
        if "```json" in json_str:
            json_str = json_str.split("```json")[1].split("```")[0].strip()
        elif "```" in json_str:
            json_str = json_str.split("```")[1].split("```")[0].strip()
        
        # Find JSON object
        start_idx = json_str.find('{')
        end_idx = json_str.rfind('}') + 1
        if start_idx != -1 and end_idx != 0:
            json_str = json_str[start_idx:end_idx]
        
        return EnhancedResumeData(**json.loads(json_str))
    except Exception as e:
        error_msg = str(e)
        print(f"Groq enhancement failed: {error_msg}")
        
        # Provide meaningful fallback based on extracted data
        fallback_corrections = ["Groq enhancement failed"]
        if "quota" in error_msg.lower() or "429" in error_msg or "rate limit" in error_msg.lower():
            fallback_corrections = ["AI quota exceeded - using basic parsing"]
        
        return EnhancedResumeData(
            resume_text=extracted_data.get("resume_text", ""),
            job_description="",
            education=", ".join([edu.get("degree", "") for edu in extracted_data.get("education", [])]),
            industry="Technology", # Default industry
            applied_job_title="Software Developer", # Default title 
            experience_years=len(extracted_data.get("work_experience", [])),
            salary_expectation=0,
            skills=extracted_data.get("extracted_skills", []),
            corrections_made=fallback_corrections
        )

# API Endpoints
@app.post("/parse-resume/", response_model=ResumeData)
async def parse_resume(file: UploadFile = File(...)):
    try:
        print(f"ðŸ“„ Processing resume: {file.filename}")
        text = await extract_text_from_pdf(file)
        print(f"âœ… Text extracted, length: {len(text)}")
        
        doc = nlp(text)
        print("âœ… spaCy processing complete")
        
        contact_info = extract_contact_info(doc)
        print(f"âœ… Contact info extracted: {contact_info}")
        
        skills = extract_skills(doc)
        print(f"âœ… Skills extracted: {len(skills)} skills found - Type: {type(skills)}")
        print(f"Skills content: {skills[:5] if skills else 'None'}")  # Show first 5 skills
        
        experience = extract_experience(doc) or []
        print(f"âœ… Experience extracted: {len(experience)} jobs found - Type: {type(experience)}")
        if experience:
            print(f"First experience item type: {type(experience[0])}")
            print("First experience item keys:", list(experience[0].keys()) if isinstance(experience[0], dict) else "Not a dict")
        
        education = extract_education(doc) or []
        print(f"âœ… Education extracted: {len(education)} entries found - Type: {type(education)}")
        if education:
            print(f"First education item type: {type(education[0])}")
            print("First education item keys:", list(education[0].keys()) if isinstance(education[0], dict) else "Not a dict")
        
        extracted_data = {
            "resume_text": text[:5000],
            "education": education,
            "work_experience": experience,
            "extracted_skills": skills,
            "contact_info": contact_info
        }
        print("âœ… Initial data extraction complete")
        
        enhanced_data = await enhance_resume_with_groq(text, extracted_data)
        print(f"âœ… Groq enhancement complete - Type: {type(enhanced_data)}")
        print(f"Enhanced skills type: {type(enhanced_data.skills)}")
        print(f"Enhanced skills content: {enhanced_data.skills[:5] if enhanced_data.skills else 'None'}")
        
        # Create the result directly without database storage first
        # Ensure skills is a list
        final_skills = enhanced_data.skills if isinstance(enhanced_data.skills, list) else []
        print(f"Final skills type: {type(final_skills)}, content: {final_skills}")
        
        try:
            result = ResumeData(
                name=contact_info.get("name", ""),
                email=contact_info.get("email", ""),
                phone=contact_info.get("phone", ""),
                resume_text=text,
                extracted_skills=final_skills,
                work_experience=experience,
                education=education,
                enhanced_data=enhanced_data
            )
            print("âœ… ResumeData object created successfully")
            return result
        except Exception as create_error:
            print(f"âŒ Error creating ResumeData object: {create_error}")
            print(f"âŒ Contact info: {contact_info}")
            print(f"âŒ Enhanced data: {enhanced_data}")
            print(f"âŒ Skills: {final_skills}")
            print(f"âŒ Experience: {experience}")
            print(f"âŒ Education: {education}")
            raise HTTPException(status_code=500, detail=f"Error creating resume data: {str(create_error)}")
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Parse resume error: {str(e)}")
        print(f"âŒ Error type: {type(e)}")
        import traceback
        print(f"âŒ Traceback: {traceback.format_exc()}")
        raise HTTPException(500, detail=str(e))

@app.post("/apply-job/")
async def apply_to_job(
    job_id: str = Form(...),
    candidate_id: str = Form(...),
    cover_letter: str = Form(""),  # Make cover_letter optional with default empty string
    full_name: str = Form(...),
    email: str = Form(...),
    phone: str = Form(""),  # Make phone optional with default empty string
    skills: str = Form(...),
    education: str = Form(...),
    experience: str = Form(...),
    certifications: str = Form(...),
    languages: str = Form(...),
    file: UploadFile = File(...),
    # Optional fields from frontend
    phone_number: str = Form(None),
    location: str = Form(None),
    linkedin_url: str = Form(None),
    github_url: str = Form(None),
    portfolio_url: str = Form(None)
):
    try:
        print(f"ðŸ“ [DEBUG] Job application request received")
        print(f"ðŸ“ [DEBUG] job_id: {job_id}")
        print(f"ðŸ“ [DEBUG] candidate_id: {candidate_id}")
        print(f"ðŸ“ [DEBUG] full_name: {full_name}")
        print(f"ðŸ“ [DEBUG] email: {email}")
        print(f"ðŸ“ [DEBUG] phone: {phone}")
        print(f"ðŸ“ [DEBUG] file: {file.filename if file else 'None'}")
        print(f"ðŸ“ [DEBUG] skills: {skills}")
        print(f"ðŸ“ [DEBUG] education: {education}")
        print(f"ðŸ“ [DEBUG] experience: {experience}")
        print(f"ðŸ“ [DEBUG] certifications: {certifications}")
        print(f"ðŸ“ [DEBUG] languages: {languages}")
        
        if not database_connected:
            raise HTTPException(status_code=503, detail="Database not configured - job applications not available")
        
        print(f"ðŸ“ Processing job application for job_id: {job_id}")
        print(f"ðŸ‘¤ Candidate: {full_name} ({email})")
        
        # Check if job exists
        job = await mongodb.find_one("jobs", {"id": job_id})
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        
        # Check if candidate has already applied to this job
        existing_application = await mongodb.find_one("applications", {
            "job_id": job_id,
            "candidate_id": candidate_id
        })
        if existing_application:
            raise HTTPException(
                status_code=409, 
                detail="You have already applied to this job"
            )
        
        # Parse JSON fields
        import json
        try:
            skills_list = json.loads(skills) if skills else []
            education_list = json.loads(education) if education else []
            experience_list = json.loads(experience) if experience else []
            certifications_list = json.loads(certifications) if certifications else []
            languages_list = json.loads(languages) if languages else []
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parsing error: {e}")
            skills_list = []
            education_list = []
            experience_list = []
            certifications_list = []
            languages_list = []
        
        # Process and store resume data using the same logic as ResumeParserTest
        resume_filename = None
        extracted_resume_id = None
        
        if file:
            resume_filename = f"resume_{candidate_id}_{job_id}_{file.filename}"
            
            try:
                print(f"ðŸ“„ Processing resume for job application: {file.filename}")
                
                # Extract text from the resume
                text = await extract_text_from_pdf(file)
                print(f"âœ… Text extracted for job application, length: {len(text)}")
                
                # Process with spaCy
                doc = nlp(text)
                
                # Extract information
                contact_info = extract_contact_info(doc)
                extracted_skills = extract_skills(doc)
                experience = extract_experience(doc) or []
                education = extract_education(doc) or []
                
                # Prepare extracted data for Groq enhancement
                extracted_data = {
                    "resume_text": text[:5000],
                    "education": education,
                    "work_experience": experience,
                    "extracted_skills": extracted_skills,
                    "contact_info": contact_info
                }
                
                # Enhance with Groq
                enhanced_data = await enhance_resume_with_groq(text, extracted_data)
                
                # Prepare resume data for storage (similar to ResumeParserTest format)
                processed_data = {
                    "personal_info": {
                        "name": contact_info.get("name", full_name),
                        "email": contact_info.get("email", email),
                        "phone": contact_info.get("phone", phone),
                    },
                    "skills": {
                        "technical_skills": enhanced_data.skills if isinstance(enhanced_data.skills, list) else [],
                        "all_skills": extracted_skills
                    },
                    "education": education,
                    "experience": experience,
                    "enhanced_data": {
                        "industry": enhanced_data.industry,
                        "job_title": enhanced_data.applied_job_title,
                        "experience_years": enhanced_data.experience_years,
                        "salary_expectation": enhanced_data.salary_expectation,
                        "corrections_made": enhanced_data.corrections_made
                    }
                }
                
                # Store in extracted_resume_data collection
                try:
                    # Check if resume already exists
                    existing_resume = await mongodb.find_one("extracted_resume_data", {
                        "personal_info.email": contact_info.get("email", email)
                    })
                    
                    if not existing_resume:
                        resume_doc = {
                            **processed_data,
                            "raw_text": text,
                            "uploaded_at": datetime.now().isoformat(),
                            "filename": file.filename,
                            "id": str(uuid.uuid4()),
                            "created_at": datetime.now().isoformat(),
                            "source": "job_application",
                            "job_id": job_id,
                            "candidate_id": candidate_id
                        }
                        
                        extracted_resume_id = await mongodb.insert_one("extracted_resume_data", resume_doc)
                        print(f"âœ… Resume data stored in extracted_resume_data: {extracted_resume_id}")
                    else:
                        extracted_resume_id = existing_resume.get("id")
                        print(f"Resume already exists in extracted_resume_data: {extracted_resume_id}")
                        
                except Exception as e:
                    print(f"âŒ Failed to store resume data: {str(e)}")
                    
            except Exception as e:
                print(f"âŒ Resume processing error: {str(e)}")
                # Continue with application even if resume processing fails
                
        application_data = {
            "id": str(uuid.uuid4()),
            "candidate_id": candidate_id,
            "user_id": candidate_id,  # Use candidate_id as user_id
            "job_id": job_id,
            "cover_letter": cover_letter,
            "full_name": full_name,
            "email": email,
            "phone": phone,
            "skills": skills_list,
            "education": education_list,
            "experience": experience_list,
            "certifications": certifications_list,
            "languages": languages_list,
            "resume_filename": resume_filename,
            "extracted_resume_id": extracted_resume_id,  # Link to extracted resume data
            # Additional candidate profile data
            "phone_number": phone_number,
            "location": location,
            "linkedin_url": linkedin_url,
            "github_url": github_url,
            "portfolio_url": portfolio_url,
            "applied_at": datetime.now().isoformat(),
            "status": "submitted"
        }
        
        application_id = await mongodb.insert_one("applications", application_data)
        
        print(f"âœ… Application submitted successfully: {application_id}")
        
        return {
            "message": "Application submitted successfully", 
            "application_id": str(application_id),
            "status": "success"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Application submission error: {str(e)}")
        import traceback
        print(f"âŒ Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Application submission failed: {str(e)}")

@app.post("/analyze-candidate")
async def analyze_candidate(request: dict):
    """
    Analyze a candidate's resume using Gemini to identify strengths and weaknesses.
    """
    try:
        candidate_data = request.get("candidate_data")
        if not candidate_data:
            return {"error": "No candidate data provided"}
        
        # Prepare prompt for Gemini
        prompt = f"""
        Analyze the following candidate information and provide:
        1. Three key strengths of the candidate
        2. Three areas for improvement
        3. Overall suitability for tech roles
        
        Make the analysis concise and specific to their skills and experience.
        
        Candidate Data:
        {candidate_data}
        
        Format your response as JSON with these keys:
        - strengths: [array of strengths]
        - improvements: [array of areas for improvement]
        - suitability: A brief assessment of their fit for tech roles
        """
        
        # Generate response from Groq
        if not client:
            return {
                "analysis": {
                    "strengths": ["Basic parsing available", "Resume submitted successfully", "Ready for manual review"],
                    "improvements": ["AI analysis temporarily unavailable", "Manual review recommended", "Try again later for detailed insights"],
                    "suitability": "Manual review required - AI analysis not available"
                }
            }
        
        try:
            response = client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[
                    {"role": "system", "content": "You are an expert HR analyst. Return only valid JSON with the requested structure."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1500
            )
            
            # Parse the response
            analysis = response.choices[0].message.content
            # Clean up the response if it contains markdown code blocks
            if "```json" in analysis:
                analysis = analysis.split("```json")[1].split("```")[0].strip()
            elif "```" in analysis:
                analysis = analysis.split("```")[1].split("```")[0].strip()
            
            # Try to parse as JSON
            try:
                parsed_analysis = json.loads(analysis)
                return {"analysis": parsed_analysis}
            except:
                # If JSON parsing fails, return as text
                return {"analysis": analysis}
                
        except Exception as groq_error:
            error_msg = str(groq_error)
            if "quota" in error_msg.lower() or "429" in error_msg or "rate limit" in error_msg.lower():
                return {
                    "analysis": {
                        "strengths": ["Resume processed successfully", "Basic skills extracted", "Ready for manual review"],
                        "improvements": ["AI quota exceeded - detailed analysis unavailable", "Manual review recommended", "Try again later for AI insights"],
                        "suitability": "Quota exceeded - manual review required for detailed assessment"
                    }
                }
            return {"error": f"Failed to parse Gemini response: {error_msg}"}
            
    except Exception as e:
        return {"error": f"Analysis failed: {str(e)}"}

# Get applications for a job (HR view)
@app.get("/api/job-applications/{job_id}")
async def get_job_applications(job_id: str):
    """Get all applications for a specific job"""
    try:
        if not database_connected:
            raise HTTPException(status_code=500, detail="Database not connected")
        
        applications = await mongodb.find_many("applications", {"job_id": job_id})
        
        # Enrich with candidate details
        enriched_applications = []
        for app in applications:
            # Get candidate profile
            candidate = await mongodb.find_one("candidate_profiles", {"user_id": app["candidate_id"]})
            if candidate:
                app["candidate_details"] = candidate
            enriched_applications.append(app)
        
        return {"applications": enriched_applications}
        
    except Exception as e:
        print(f"âŒ Error fetching job applications: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch job applications: {str(e)}")

# Get single job by ID
@app.get("/api/jobs/{job_id}")
async def get_job_by_id(job_id: str):
    """Get a single job by ID"""
    try:
        if not database_connected:
            raise HTTPException(status_code=500, detail="Database not connected")
        
        job = await mongodb.find_one("jobs", {"id": job_id})
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        
        return job
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error fetching job: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch job: {str(e)}")

def normalize_resume_data(raw_ai_response: Dict[str, Any]) -> Dict[str, Any]:
    """
    Normalize AI response to a consistent resume format
    """
    normalized = {
        "personal_info": {},
        "education_history": [],
        "work_experience": [],
        "skills": {
            "technical": [],
            "soft": []
        },
        "certifications": [],
        "achievements": [],
        "projects": [],
        "hobbies": [],
        "ats_score": 0
    }
    
    # Extract personal information
    personal_fields = raw_ai_response.get("personal_information", {}) or raw_ai_response.get("personal_info", {}) or {}
    
    # Handle various field name variations
    normalized["personal_info"]["name"] = (
        personal_fields.get("name") or 
        personal_fields.get("full_name") or 
        raw_ai_response.get("name") or 
        raw_ai_response.get("full_name") or 
        ""
    )
    
    normalized["personal_info"]["email"] = (
        personal_fields.get("email") or 
        raw_ai_response.get("email") or 
        ""
    )
    
    normalized["personal_info"]["phone"] = (
        personal_fields.get("phone") or 
        personal_fields.get("phone_number") or 
        raw_ai_response.get("phone") or 
        raw_ai_response.get("phone_number") or 
        ""
    )
    
    normalized["personal_info"]["address"] = (
        personal_fields.get("address") or 
        personal_fields.get("location") or 
        raw_ai_response.get("address") or 
        raw_ai_response.get("location") or 
        ""
    )
    
    # Add social links
    for field in ["linkedin", "github", "twitter", "gfg", "leetcode", "codechef", "hackerrank", "portfolio"]:
        value = personal_fields.get(field) or raw_ai_response.get(field) or ""
        if value:
            normalized["personal_info"][field] = value
    
    # Education history
    education = raw_ai_response.get("education") or raw_ai_response.get("education_history") or []
    if isinstance(education, list):
        for edu in education:
            if isinstance(edu, dict):
                normalized["education_history"].append({
                    "institution": edu.get("institution") or edu.get("school") or "",
                    "degree": edu.get("degree") or edu.get("program") or "",
                    "cgpa": edu.get("cgpa") or edu.get("gpa") or "",
                    "percentage": edu.get("percentage") or "",
                    "year": edu.get("year") or edu.get("graduation_year") or "Not specified"
                })
    
    # Work experience
    experience = raw_ai_response.get("work_experience") or raw_ai_response.get("experience") or []
    if isinstance(experience, list):
        for exp in experience:
            if isinstance(exp, dict):
                normalized["work_experience"].append({
                    "company": exp.get("company") or exp.get("organization") or "",
                    "position": exp.get("position") or exp.get("role") or exp.get("title") or "",
                    "duration": exp.get("duration") or exp.get("period") or "",
                    "months": exp.get("months") or ""
                })
    
    # Skills
    skills = raw_ai_response.get("skills", {})
    if isinstance(skills, dict):
        # Technical skills
        tech_skills = (
            skills.get("technical") or 
            skills.get("technical_skills") or 
            skills.get("hard_skills") or 
            []
        )
        if isinstance(tech_skills, list):
            normalized["skills"]["technical"] = tech_skills
        
        # Soft skills
        soft_skills = (
            skills.get("soft") or 
            skills.get("soft_skills") or 
            skills.get("interpersonal") or 
            []
        )
        if isinstance(soft_skills, list):
            normalized["skills"]["soft"] = soft_skills
    elif isinstance(skills, list):
        # If skills is just a list, treat as technical skills
        normalized["skills"]["technical"] = skills
    
    # Certifications
    certs = raw_ai_response.get("certifications") or []
    if isinstance(certs, list):
        for cert in certs:
            if isinstance(cert, dict):
                normalized["certifications"].append({
                    "name": cert.get("name") or cert.get("title") or "",
                    "issuer": cert.get("issuer") or cert.get("organization") or "",
                    "year": cert.get("year") or cert.get("date") or ""
                })
            elif isinstance(cert, str):
                normalized["certifications"].append({
                    "name": cert,
                    "issuer": "",
                    "year": ""
                })
    
    # Achievements
    achievements = raw_ai_response.get("achievements") or raw_ai_response.get("awards") or []
    if isinstance(achievements, list):
        for achievement in achievements:
            if isinstance(achievement, dict):
                normalized["achievements"].append({
                    "achievement": achievement.get("achievement") or achievement.get("description") or "",
                    "year": achievement.get("year") or achievement.get("date") or "Not specified"
                })
            elif isinstance(achievement, str):
                normalized["achievements"].append({
                    "achievement": achievement,
                    "year": "Not specified"
                })
    
    # Projects
    projects = raw_ai_response.get("projects") or []
    if isinstance(projects, list):
        for project in projects:
            if isinstance(project, dict):
                normalized["projects"].append({
                    "name": project.get("name") or project.get("title") or "",
                    "technologies": project.get("technologies") or project.get("tech_stack") or "",
                    "description": project.get("description") or ""
                })
            elif isinstance(project, str):
                normalized["projects"].append({
                    "name": project,
                    "technologies": "",
                    "description": ""
                })
    
    # Hobbies
    hobbies = raw_ai_response.get("hobbies") or raw_ai_response.get("interests") or []
    if isinstance(hobbies, list):
        normalized["hobbies"] = hobbies
    
    # ATS Score
    normalized["ats_score"] = raw_ai_response.get("ats_score") or 0
    
    return normalized

# Authentication Endpoints
@app.post("/auth/signup", response_model=UserResponse)
async def sign_up(user_data: UserSignUp):
    """User registration endpoint"""
    try:
        if not database_connected:
            raise HTTPException(status_code=500, detail="Database not connected")
        
        # Check if user already exists
        existing_user = await mongodb.find_one("users", {"email": user_data.email})
        if existing_user:
            raise HTTPException(status_code=400, detail="User with this email already exists")
        
        # Create new user
        user_id = str(uuid.uuid4())
        hashed_password = hash_password(user_data.password)
        
        new_user = {
            "id": user_id,
            "email": user_data.email,
            "password": hashed_password,
            "first_name": user_data.first_name,
            "last_name": user_data.last_name,
            "role": user_data.role,
            "company_id": user_data.company_id,
            "company_name": user_data.company_name,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        # Save to MongoDB
        await mongodb.insert_one("users", new_user)
        print(f"âœ… User created: {user_data.email} with role {user_data.role}")
        
        # Return user data (without password)
        return UserResponse(
            id=user_id,
            email=user_data.email,
            first_name=user_data.first_name,
            last_name=user_data.last_name,
            role=user_data.role,
            created_at=new_user["created_at"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Sign up error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Registration failed: {str(e)}")

@app.post("/auth/signin")
async def sign_in(credentials: UserSignIn):
    """User authentication endpoint"""
    try:
        if not database_connected:
            raise HTTPException(status_code=500, detail="Database not connected")
        
        # Find user by email
        user = await mongodb.find_one("users", {"email": credentials.email})
        if not user:
            raise HTTPException(status_code=401, detail="Invalid email or password")
        
        # Check password
        hashed_password = hash_password(credentials.password)
        if user["password"] != hashed_password:
            raise HTTPException(status_code=401, detail="Invalid email or password")
        
        # Generate session token
        token = generate_token()
        
        # Update user with last login
        await mongodb.update_one("users", 
                                {"id": user["id"]}, 
                                {"last_login": datetime.now().isoformat()})
        
        print(f"âœ… User signed in: {credentials.email}")
        
        # Return user data and token
        return {
            "user": {
                "id": user["id"],
                "email": user["email"],
                "first_name": user.get("first_name"),
                "last_name": user.get("last_name"),
                "role": user["role"],
                "company_id": user.get("company_id"),
                "company_name": user.get("company_name"),
                "created_at": user["created_at"]
            },
            "token": token
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Sign in error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Authentication failed: {str(e)}")

@app.get("/auth/user")
async def get_current_user(request: Request):
    """Get current user data (requires authentication header)"""
    try:
        # In a real app, you'd validate the token from Authorization header
        # For now, just return a success response
        return {"message": "User endpoint available"}
        
    except Exception as e:
        print(f"âŒ Get user error: {str(e)}")
        raise HTTPException(status_code=401, detail="Authentication required")

# Resume Processing Endpoints
@app.post("/send-data", response_model=RawResumeData)
async def send_data(file: UploadFile = File(...)):
    """Endpoint that sends raw resume data to Groq for processing"""
    try:
        # 1. Extract raw text from PDF
        raw_text = await extract_raw_text_from_pdf(file)
        
        # 2. Send directly to Groq for processing
        raw_processed_data = await process_with_groq(raw_text)
        
        # 3. Normalize the data to consistent format
        processed_data = normalize_resume_data(raw_processed_data)
        
        # 4. Append to JSON file if not already exists (legacy support)
        was_added_to_file = append_resume_to_file(processed_data)
        
        # 5. Also save to MongoDB if connected
        was_added_to_db = False
        if database_connected:
            try:
                # Check if resume already exists in MongoDB
                existing_resume = None
                email = processed_data.get("personal_info", {}).get("email")
                name = processed_data.get("personal_info", {}).get("name")
                
                if email:
                    existing_resume = await mongodb.find_one("extracted_resume_data", {"personal_info.email": email})
                elif name:
                    existing_resume = await mongodb.find_one("extracted_resume_data", {"personal_info.name": name})
                
                if not existing_resume:
                    # Add metadata
                    resume_doc = {
                        **processed_data,
                        "raw_text": raw_text,
                        "uploaded_at": datetime.now().isoformat(),
                        "filename": file.filename,
                        "id": str(uuid.uuid4()),
                        "created_at": datetime.now().isoformat()
                    }
                    
                    # Save to MongoDB
                    await mongodb.insert_one("extracted_resume_data", resume_doc)
                    print(f"âœ… Resume saved to MongoDB: {email or name or 'Unknown'}")
                    was_added_to_db = True
                else:
                    print(f"Resume already exists in MongoDB: {email or name or 'Unknown'}")
                    
            except Exception as e:
                print(f"Failed to save resume to MongoDB: {str(e)}")
        
        response_data = RawResumeData(
            raw_text=raw_text[:1000] + "... [truncated]",
            processed_data=processed_data
        )
        
        # Add a note about whether the resume was added or already existed
        if not was_added_to_file and not was_added_to_db:
            response_data.processed_data["note"] = "Resume already exists in database"
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Resume processing failed: {str(e)}"
        )
# Load spaCy model
try:
    nlp = spacy.load("en_core_web_sm")
    print("âœ… spaCy model loaded successfully")
except Exception as e:
    print(f"âŒ spaCy model loading failed: {e}")
    raise

# Initialize MongoDB
database_connected = False
async def startup_event():
    """Initialize database connection on startup"""
    global database_connected
    database_connected = await init_database()
    if database_connected:
        print("âœ… MongoDB connection successful")
    else:
        print("âš ï¸ MongoDB not configured - running without database")

async def shutdown_event():
    """Close database connection on shutdown"""
    await close_database()

# Constants
SKILL_BLACKLIST = {
    "and", "the", "with", "using", "via", "from", "to", 
    "in", "on", "at", "for", "my", "our", "we", "i",
    "|", "", " ", "  ", "-", "â€¢", ":", ";", ",", "."
}

TECHNICAL_SKILLS = [
    "Python", "Java", "JavaScript", "C++", "SQL", "NoSQL", 
    "HTML", "CSS", "React", "Angular", "Node.js", "Django",
    "Flask", "TensorFlow", "PyTorch", "Machine Learning",
    "Data Science", "Data Analysis", "Android Development",
    "IoT", "Cloud Computing", "AWS", "Azure", "Git",
    "REST API", "GraphQL", "Docker", "Kubernetes",
    "Computer Engineering", "Research", "Web Development"
]

# Models
class EnhancedResumeData(BaseModel):
    resume_text: str
    job_description: str
    education: str
    industry: str
    applied_job_title: str
    experience_years: float
    salary_expectation: float
    skills: List[str]
    corrections_made: List[str] = Field(default_factory=list)

class ResumeData(BaseModel):
    name: str
    email: Optional[str] = None
    phone: Optional[str] = None
    resume_text: str
    extracted_skills: List[str]
    work_experience: List[Dict[str, Any]]
    education: List[Dict[str, Any]]
    certifications: Optional[List[str]] = None
    enhanced_data: Optional[EnhancedResumeData] = None

class JobApplication(BaseModel):
    job_id: str
    candidate_id: str

class JobWithCandidates(BaseModel):
    job: Dict[str, Any]
    candidates: List[ResumeData]

# Authentication models
class UserSignUp(BaseModel):
    email: str  # Changed from EmailStr to str to avoid validation issues
    password: str
    first_name: Optional[str] = None
    last_name: Optional[str] = None
    role: str = "candidate"  # candidate or hr_user
    company_id: Optional[str] = None
    company_name: Optional[str] = None

class UserSignIn(BaseModel):
    email: str  # Changed from EmailStr to str to avoid validation issues
    password: str

class UserResponse(BaseModel):
    id: str
    email: str
    first_name: Optional[str] = None
    last_name: Optional[str] = None
    role: str
    created_at: str

# Simple JWT-like token (in production, use proper JWT)
import hashlib
import secrets

def hash_password(password: str) -> str:
    """Simple password hashing"""
    return hashlib.sha256(password.encode()).hexdigest()

def generate_token() -> str:
    """Generate a simple session token"""
    return secrets.token_urlsafe(32)

# Helper Functions
def initialize_nlp_components():
    skill_patterns = list(nlp.pipe([
        skill for skill in TECHNICAL_SKILLS 
        if len(skill.split()) < 3
    ]))
    skill_matcher = PhraseMatcher(nlp.vocab)
    skill_matcher.add("SKILL", skill_patterns)

    experience_matcher = Matcher(nlp.vocab)
    experience_patterns = [
        [{"POS": "PROPN", "OP": "+"}, {"LOWER": "at"}, {"POS": "PROPN", "OP": "+"}],
        [{"POS": "PROPN", "OP": "+"}, {"LOWER": ","}, {"LOWER": "inc"}],
        [{"POS": "PROPN", "OP": "+"}, {"LOWER": "company"}],
    ]
    experience_matcher.add("EXPERIENCE", experience_patterns)

    return skill_matcher, experience_matcher

skill_matcher, experience_matcher = initialize_nlp_components()

def is_valid_skill(text):
    return (
        len(text) > 2 and
        not any(char.isdigit() for char in text) and
        text.lower() not in SKILL_BLACKLIST and
        not text.isspace() and
        not text.endswith((".", ",", ";", ":"))
    )

def extract_skills(doc) -> List[str]:
    skills = set()
    matches = skill_matcher(doc)
    for match_id, start, end in matches:
        skill = doc[start:end].text
        if is_valid_skill(skill):
            skills.add(skill)
    
    for sent in doc.sents:
        if any(kw in sent.text.lower() for kw in ["skill", "experience", "proficient"]):
            for token in sent:
                if (token.pos_ in ["NOUN", "PROPN"] and is_valid_skill(token.text)):
                    skills.add(token.text)
    
    return sorted({re.sub(r'[^\w\s]', '', s).strip() for s in skills if is_valid_skill(s)})

def extract_from_section(doc, section_title):
    section_content = []
    found_section = False
    for sent in doc.sents:
        if section_title.lower() in sent.text.lower():
            found_section = True
            continue
        if found_section and any(kw in sent.text.lower() for kw in ["experience", "education"]):
            break
        if found_section:
            section_content.append(sent.text)
    return " ".join(section_content)

async def extract_text_from_pdf(file: UploadFile) -> str:
    try:
        if file.content_type != "application/pdf":
            raise ValueError("Only PDF files are accepted")
            
        contents = await file.read()
        if not contents:
            raise ValueError("Empty file uploaded")
            
        if len(contents) > 5 * 1024 * 1024:
            raise ValueError("File too large (max 5MB)")
            
        pdf_file = io.BytesIO(contents)
        reader = PyPDF2.PdfReader(pdf_file)
        if len(reader.pages) == 0:
            raise ValueError("PDF contains no pages")
            
        text = "\n".join([page.extract_text() or "" for page in reader.pages])
        if not text.strip():
            raise ValueError("No text could be extracted from PDF")
            
        file.file.seek(0)
        return text
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"PDF processing failed: {str(e)}")

async def upload_resume_to_storage(file: UploadFile) -> str:
    """
    Mock file upload - returns a placeholder URL
    In production, you would upload to a cloud storage service like AWS S3, Google Cloud Storage, etc.
    """
    try:
        file_extension = os.path.splitext(file.filename)[1]
        unique_filename = f"{uuid.uuid4()}{file_extension}"
        
        # For now, return a placeholder URL
        # In production, implement actual file upload to your preferred storage service
        placeholder_url = f"https://storage.example.com/resumes/{unique_filename}"
        
        return placeholder_url
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to upload resume: {str(e)}")

def extract_contact_info(doc) -> Dict[str, str]:
    contact_info = {"name": "", "email": "", "phone": ""}
    for ent in doc.ents:
        if ent.label_ == "PERSON":
            contact_info["name"] = ent.text
            break
    
    emails = re.findall(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", doc.text)
    if emails:
        contact_info["email"] = emails[0]
    
    phones = re.findall(r"(\+?\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}", doc.text)
    if phones:
        contact_info["phone"] = phones[0]
    
    return contact_info

def extract_experience(doc) -> List[Dict[str, Any]]:
    experiences = []
    current_position = None
    current_dates = None
    
    for sent in doc.sents:
        if (" at " in sent.text or " for " in sent.text or 
            " intern " in sent.text.lower() or "internship" in sent.text.lower()):
            position = sent.text.split(" at ")[0] if " at " in sent.text else sent.text
            current_position = position.split(" for ")[0] if " for " in position else position
        
        date_matches = re.findall(
            r"((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]\s\d{4}|\d{4}).?((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s\d{4}|\d{4}|Present|Current)", 
            sent.text, 
            re.IGNORECASE
        )
        if date_matches:
            current_dates = f"{date_matches[0][0]} - {date_matches[0][2]}"
        
        if current_position and current_dates:
            experiences.append({
                "position": current_position.strip(),
                "duration": current_dates,
                "description": sent.text,
                "type": "internship" if "intern" in current_position.lower() else "work"
            })
            current_position = current_dates = None
    
    return experiences

def extract_education(doc) -> List[Dict[str, Any]]:
    education = []
    current_degree = None
    current_school = None
    
    for sent in doc.sents:
        if any(kw in sent.text for kw in ["Bachelor", "Master", "PhD", "Doctorate"]):
            current_degree = sent.text.split(" in ")[0] if " in " in sent.text else sent.text
        
        if any(kw in sent.text for kw in ["University", "College", "Institute"]):
            current_school = sent.text
        
        if current_degree and current_school:
            education.append({
                "degree": current_degree,
                "institution": current_school
            })
            current_degree = current_school = None
    
    return education

async def enhance_resume_with_groq(raw_text: str, extracted_data: dict) -> EnhancedResumeData:
    print(f"ðŸ” Enhancing resume with Groq...")
    print(f"ðŸ“ Raw text length: {len(raw_text)}")
    print(f"ðŸ“Š Extracted data keys: {list(extracted_data.keys())}")
    
    # Safely convert extracted_data to JSON
    try:
        json_str = json.dumps(extracted_data, indent=2, default=str)
        print(f"âœ… JSON conversion successful")
    except Exception as json_error:
        print(f"âŒ JSON conversion failed: {json_error}")
        json_str = str(extracted_data)
    
    prompt = f"""
    Analyze this resume and enhance the extracted data:
    
    RAW TEXT EXCERPT:
    {raw_text[:3000]}
    
    CURRENT EXTRACTION:
    {json_str}
    
    Please return enhanced data in this exact format:
    {{
        "resume_text": "Enhanced professional summary",
        "job_description": "Matched job description",
        "education": "Standardized education",
        "industry": "Detected industry",
        "applied_job_title": "Suggested job title",
        "experience_years": X.X,
        "salary_expectation": XXXXX.X,
        "skills": ["Standardized", "Skills"],
        "corrections_made": ["List of corrections"]
    }}
    """
    
    try:
        if not client:
            print("Groq not available, using fallback data")
            return EnhancedResumeData(
                resume_text=extracted_data.get("resume_text", ""),
                job_description="",
                education=", ".join([edu.get("degree", "") for edu in extracted_data.get("education", [])]),
                industry="Technology", # Default industry
                applied_job_title="Software Developer", # Default title
                experience_years=len(extracted_data.get("work_experience", [])),
                salary_expectation=0,
                skills=extracted_data.get("extracted_skills", []),
                corrections_made=["Using basic parsing - Groq not available"]
            )
        
        response = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "You are an expert resume analyzer. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=2000
        )
        
        json_str = response.choices[0].message.content
        # Clean the response if it contains markdown code blocks
        if "```json" in json_str:
            json_str = json_str.split("```json")[1].split("```")[0].strip()
        elif "```" in json_str:
            json_str = json_str.split("```")[1].split("```")[0].strip()
        
        # Find JSON object
        start_idx = json_str.find('{')
        end_idx = json_str.rfind('}') + 1
        if start_idx != -1 and end_idx != 0:
            json_str = json_str[start_idx:end_idx]
        
        return EnhancedResumeData(**json.loads(json_str))
    except Exception as e:
        error_msg = str(e)
        print(f"Groq enhancement failed: {error_msg}")
        
        # Provide meaningful fallback based on extracted data
        fallback_corrections = ["Groq enhancement failed"]
        if "quota" in error_msg.lower() or "429" in error_msg or "rate limit" in error_msg.lower():
            fallback_corrections = ["AI quota exceeded - using basic parsing"]
        
        return EnhancedResumeData(
            resume_text=extracted_data.get("resume_text", ""),
            job_description="",
            education=", ".join([edu.get("degree", "") for edu in extracted_data.get("education", [])]),
            industry="Technology", # Default industry
            applied_job_title="Software Developer", # Default title 
            experience_years=len(extracted_data.get("work_experience", [])),
            salary_expectation=0,
            skills=extracted_data.get("extracted_skills", []),
            corrections_made=fallback_corrections
        )

# API Endpoints
@app.post("/parse-resume/", response_model=ResumeData)
async def parse_resume(file: UploadFile = File(...)):
    try:
        print(f"ðŸ“„ Processing resume: {file.filename}")
        text = await extract_text_from_pdf(file)
        print(f"âœ… Text extracted, length: {len(text)}")
        
        doc = nlp(text)
        print("âœ… spaCy processing complete")
        
        contact_info = extract_contact_info(doc)
        print(f"âœ… Contact info extracted: {contact_info}")
        
        skills = extract_skills(doc)
        print(f"âœ… Skills extracted: {len(skills)} skills found - Type: {type(skills)}")
        print(f"Skills content: {skills[:5] if skills else 'None'}")  # Show first 5 skills
        
        experience = extract_experience(doc) or []
        print(f"âœ… Experience extracted: {len(experience)} jobs found - Type: {type(experience)}")
        if experience:
            print(f"First experience item type: {type(experience[0])}")
            print("First experience item keys:", list(experience[0].keys()) if isinstance(experience[0], dict) else "Not a dict")
        
        education = extract_education(doc) or []
        print(f"âœ… Education extracted: {len(education)} entries found - Type: {type(education)}")
        if education:
            print(f"First education item type: {type(education[0])}")
            print("First education item keys:", list(education[0].keys()) if isinstance(education[0], dict) else "Not a dict")
        
        extracted_data = {
            "resume_text": text[:5000],
            "education": education,
            "work_experience": experience,
            "extracted_skills": skills,
            "contact_info": contact_info
        }
        print("âœ… Initial data extraction complete")
        
        enhanced_data = await enhance_resume_with_groq(text, extracted_data)
        print(f"âœ… Groq enhancement complete - Type: {type(enhanced_data)}")
        print(f"Enhanced skills type: {type(enhanced_data.skills)}")
        print(f"Enhanced skills content: {enhanced_data.skills[:5] if enhanced_data.skills else 'None'}")
        
        # Create the result directly without database storage first
        # Ensure skills is a list
        final_skills = enhanced_data.skills if isinstance(enhanced_data.skills, list) else []
        print(f"Final skills type: {type(final_skills)}, content: {final_skills}")
        
        try:
            result = ResumeData(
                name=contact_info.get("name", ""),
                email=contact_info.get("email", ""),
                phone=contact_info.get("phone", ""),
                resume_text=text,
                extracted_skills=final_skills,
                work_experience=experience,
                education=education,
                enhanced_data=enhanced_data
            )
            print("âœ… ResumeData object created successfully")
            return result
        except Exception as create_error:
            print(f"âŒ Error creating ResumeData object: {create_error}")
            print(f"âŒ Contact info: {contact_info}")
            print(f"âŒ Enhanced data: {enhanced_data}")
            print(f"âŒ Skills: {final_skills}")
            print(f"âŒ Experience: {experience}")
            print(f"âŒ Education: {education}")
            raise HTTPException(status_code=500, detail=f"Error creating resume data: {str(create_error)}")
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Parse resume error: {str(e)}")
        print(f"âŒ Error type: {type(e)}")
        import traceback
        print(f"âŒ Traceback: {traceback.format_exc()}")
        raise HTTPException(500, detail=str(e))

@app.post("/apply-job/")
async def apply_to_job(
    job_id: str = Form(...),
    candidate_id: str = Form(...),
    cover_letter: str = Form(""),  # Make cover_letter optional with default empty string
    full_name: str = Form(...),
    email: str = Form(...),
    phone: str = Form(""),  # Make phone optional with default empty string
    skills: str = Form(...),
    education: str = Form(...),
    experience: str = Form(...),
    certifications: str = Form(...),
    languages: str = Form(...),
    file: UploadFile = File(...),
    # Optional fields from frontend
    phone_number: str = Form(None),
    location: str = Form(None),
    linkedin_url: str = Form(None),
    github_url: str = Form(None),
    portfolio_url: str = Form(None)
):
    try:
        print(f"ðŸ“ [DEBUG] Job application request received")
        print(f"ðŸ“ [DEBUG] job_id: {job_id}")
        print(f"ðŸ“ [DEBUG] candidate_id: {candidate_id}")
        print(f"ðŸ“ [DEBUG] full_name: {full_name}")
        print(f"ðŸ“ [DEBUG] email: {email}")
        print(f"ðŸ“ [DEBUG] phone: {phone}")
        print(f"ðŸ“ [DEBUG] file: {file.filename if file else 'None'}")
        print(f"ðŸ“ [DEBUG] skills: {skills}")
        print(f"ðŸ“ [DEBUG] education: {education}")
        print(f"ðŸ“ [DEBUG] experience: {experience}")
        print(f"ðŸ“ [DEBUG] certifications: {certifications}")
        print(fi f   _ _ n a m e _ _   = =   " _ _ m a i n _ _ " :   i m p o r t   u v i c o r n ;   u v i c o r n . r u n ( a p p ,   h o s t = " 0 . 0 . 0 . 0 " ,   p o r t = 8 0 0 0 )  
 